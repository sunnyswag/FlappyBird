{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, max_size, batch_size, action_dim=1):\n",
    "        self.max_size = max_size\n",
    "        self.state_dim = state_dim\n",
    "        self.other_dim = action_dim + 1 + 1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state = np.empty((self.max_size, self.state_dim), dtype=np.float32)\n",
    "        self.other = np.empty((self.max_size, self.other_dim), dtype=np.float32)\n",
    "        self.size = 0\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def store(self, state, action, reward, done):\n",
    "        self.state[self.current_index] = state\n",
    "        self.other[self.current_index] = [action, reward, done]\n",
    "        self.current_index = (self.current_index + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        ptr = np.random.choice(self.size, self.batch_size)\n",
    "        return (torch.FloatTensor(self.state[ptr]).to(self.device),\n",
    "                torch.FloatTensor(self.state[ptr + 1]).to(self.device),\n",
    "                    # TODO remove reshape\n",
    "                torch.LongTensor(self.other[ptr, 0:1].reshape(-1, 1)).to(self.device),\n",
    "                torch.FloatTensor(self.other[ptr, 1:2].reshape(-1, 1)).to(self.device),\n",
    "                torch.FloatTensor(self.other[ptr, 2:].reshape(-1, 1)).to(self.device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, mid_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env_name, mid_dim = 256):\n",
    "        self.learning_rate = 1e-4\n",
    "        self.gamma = 0.99\n",
    "        self.soft_update_tau = 5e-3\n",
    "        self.episode_num = 10000\n",
    "        self.update_step = 300\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.mid_dim = mid_dim\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.max_size = 100000\n",
    "        self.batch_size = 256\n",
    "        \n",
    "        self.network = QNetwork(self.state_dim, self.mid_dim, self.action_dim).to(self.device)\n",
    "        self.target_network = deepcopy(self.network)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(self.state_dim, self.max_size, self.batch_size)\n",
    "    \n",
    "    def claculate_epsilon(self, episode):\n",
    "        min_epsilon = 0.05\n",
    "        max_epsilon = 1\n",
    "        epsilon_decay = 800\n",
    "        epsilon_episode = lambda episode : min_epsilon + np.exp(-episode / epsilon_decay)*0.95\n",
    "        \n",
    "        return epsilon_episode(episode)\n",
    "        \n",
    "    def select_action(self, episode, state):\n",
    "        if np.random.random_sample() > self.claculate_epsilon(episode):\n",
    "            return self.network(torch.FloatTensor(state).to(self.device)).argmax().detach().cpu().numpy()\n",
    "        else:\n",
    "            return self.env.action_space.sample()\n",
    "    \n",
    "    def update(self):\n",
    "        for _ in range(self.update_step):\n",
    "            with torch.no_grad():\n",
    "                state, next_state, action, reward, done = self.replay_buffer.sample_batch()\n",
    "                next_Q = self.target_network(next_state).max(dim = 1, keepdim=True)[0]\n",
    "                target = reward + done * next_Q * self.gamma\n",
    "\n",
    "            current_Q = self.network(state).gather(1, action)\n",
    "            loss = self.criterion(current_Q, target)\n",
    "            \n",
    "            self.optimizer.zero_gard()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.soft_update(self.target_network, self.network, self.soft_update_tau)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def soft_update(target_net, current_net, tau):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data.__mul__(tau) + tar.data.__mul__(1 - tau))\n",
    "        \n",
    "    def load_model(self):\n",
    "        pass\n",
    "    \n",
    "    def save_model(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    def __init__(self, plot_num):\n",
    "        self.plot_num = plot_num\n",
    "        \n",
    "    def smooth_plot(self, item, factor=10, plot_decay=350):\n",
    "        item_x = np.arange(len(item))\n",
    "        item_smooth = [np.mean(item[i:i+factor]) if i > factor else np.mean(item[0:i+1])\n",
    "                      for i in range(len(item))]\n",
    "        for i in range(len(item) // plot_decay):\n",
    "            item_x = item_x[::2]\n",
    "            item_smooth = item_smooth[::2]\n",
    "        return item_x, item_smooth\n",
    "    \n",
    "    def plot_fig(self, episode, **kwargs):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(18, 4 * self.plot_num))\n",
    "        \n",
    "        for index, key in enumerate(kwargs.keys()):\n",
    "            data = kwargs[key]\n",
    "            data_x, data_smooth = self.smooth_plot(data)\n",
    "            \n",
    "            plt.subplot(self.plot_num, 1, index + 1)\n",
    "            plt.title('episode {}. {}: {}'.format(episode, key, data_smooth[-1]))\n",
    "            plt.plot(data, label=key, color='lightsteelblue', linewidth='1')\n",
    "            plt.plot(data_x, data_smooth, label='Smothed_{}'.format(key), color='darkorange', linewidth='3')\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "agent = Agent(env_name)\n",
    "plot = Plot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c07fa6d31b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mall_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "all_rewards, all_losses, episode_steps = [], [], []\n",
    "\n",
    "for episode in range(agent.episode_num):\n",
    "    state, rewards = agent.env.reset(), 0\n",
    "    losses = []\n",
    "    for i in range(10000):\n",
    "        action = agent.select_action(episode, state)\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        agent.replay_buffer.store(state, action, reward, done)\n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            episode_steps.append(i)\n",
    "            break\n",
    "        if len(agent.replay_buffer) > agent.batch_size:\n",
    "            losses.append(agent.update())\n",
    "    all_losses.append(sum(losses)/len(losses))\n",
    "    all_rewards.append(rewards)\n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        kwargs = {\n",
    "            \"Losses\": all_losses,\n",
    "            \"Rewards\": all_rewards,\n",
    "            \"Episode_Steps\": episode_steps\n",
    "        }\n",
    "        plot.plot_fig(episode, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n - 1\n",
    "# replay_buffer = ReplayBuffer()\n",
    "print(state_dim, action_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
